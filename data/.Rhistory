}
optim(par = 2, fn = log.poissonlikelihood, n = n, method = "Brent", lower = 0, upper = 10)
optim(par = 2, fn = log.poissonlikelihood, n = n, method = "Brent", lower = 0, upper = 10)
optim(par = 2, fn = log.poissonlikelihood)
knitr::opts_chunk$set(echo = TRUE)
```{r}
# Create a function for the log of the likelihood
nllik <- function (lambda, times) -sum(dexp(times, lambda, log = TRUE))
negloglikelihood <- function (lambda, times) -sum(dexp(times, lambda, log = TRUE))
optimize(negloglikelihood, lower = 0, upper = 10, obs = times)
optimize(negloglikelihood, lower = 0, upper = 10, times = x)
negloglikelihood <- function (lambda, obs) -sum(dexp(obs, lambda, log = TRUE))
optimize(negloglikelihood, lower = 0, upper = 10, obs = x)
optimize(negloglikelihood, lower = 0, upper = 10, obs = times)
times = c(2.65871285, 8.34273228, 5.09845548, 7.15064545,
0.39974647, 0.77206050, 5.43415199, 0.36422211,
3.30789126, 0.07621921, 2.13375997, 0.06577856,
1.73557740, 0.16524304, 0.27652044)
negloglikelihood <- function (lambda, obs) -sum(dexp(obs, lambda, log = TRUE))
optimize(negloglikelihood, lower = 0, upper = 10, obs = times)
mean(times)
1/mean(times)
# To find the MLE, we minimize the negative of the log likelihood function
# which is recommended by R
negloglikelihood <- function (lambda, obs) -sum(dexp(obs, lambda, log = TRUE))
optimize(negloglikelihood, lower = 0, upper = 10, obs = times)
# using our value of Lambda, 1/mean, we find nearly the same solution as optimize
1/mean(times)
# To find the MLE, we minimize the negative of the log likelihood function
# (which finds the maximum).
negloglikelihood <- function (lambda, n) -sum(dexp(obs, lambda, log = TRUE))
optimize(negloglikelihood, lower = 0, upper = 10, n = times)
# To find the MLE, we minimize the negative of the log likelihood function
# (which finds the maximum).
negloglikelihood <- function (lambda, n) -sum(dexp(n, lambda, log = TRUE))
optimize(negloglikelihood, lower = 0, upper = 10, n = times)
plot(loglikelihood)
lam <- 1/mean(times)
loglikelihood <- function (lambda, n) sum(dexp(n, lambda, log = TRUE))
plot(loglikelihood)
lam <- 1/mean(times)
loglikelihood <- function (lambda, n) sum(dexp(n, lambda, log = TRUE))
plot(loglikelihood(lam, 15))
lam <- 1/mean(times)
loglikelihood <- function (lambda, n) -sum(dexp(n, lambda, log = TRUE))
plot(loglikelihood(lam, 15))
lam <- 1/mean(times)
loglikelihood <- function (lambda, n) sum(dexp(n, lambda, log = TRUE))
plot(loglikelihood(lam, 15))
lam <- 1/mean(times)
#
# To find the MLE, we minimize the negative of the log likelihood function
# (which finds the maximum).
negloglikelihood <- function (lambda, n) -sum(dexp(n, lambda, log = TRUE))
optimize(negloglikelihood, lower = 0, upper = 10, n = times)
?dexp
likelihood <- function(x){
lambda = 1/mean(x)
length(x) * log(lambda) - lambda * (sum(x))}
plot(likelihood(times), type="p", x = times)
likelihood <- function(x){
lambda = 1/mean(x)
length(x) * log(lambda) - lambda * (sum(x))}
plot(likelihood(times, lambda), type="p", x = times)
times = c(2.65871285, 8.34273228, 5.09845548, 7.15064545,
0.39974647, 0.77206050, 5.43415199, 0.36422211,
3.30789126, 0.07621921, 2.13375997, 0.06577856,
1.73557740, 0.16524304, 0.27652044)
likelihood <- function(x, lambda){
lambda = 1/mean(x)
length(x) * log(lambda) - lambda * (sum(x))}
plot(likelihood(times, lambda), type="p", x = times)
likelihood <- function(x, lambda){
lambda = 1/mean(x)
length(x) * log(lambda) - lambda * (sum(x))}
plot(likelihood(lambda, times), type="p", x = times)
lambda = c(1,100)
likelihood <- function(x, lambda){
lambda = 1/mean(x)
length(x) * log(lambda) - lambda * (sum(x))}
plot(likelihood(lambda, times), type="p", x = times)
lambda = c(1,100)
likelihood <- function(x, lambda){
lambda = 1/mean(x)
length(x) * log(lambda) - lambda * (sum(x))}
plot(likelihood(times, lambda), type="p", x = times)
times = c(2.65871285, 8.34273228, 5.09845548, 7.15064545,
0.39974647, 0.77206050, 5.43415199, 0.36422211,
3.30789126, 0.07621921, 2.13375997, 0.06577856,
1.73557740, 0.16524304, 0.27652044)
#likelihood <- function(x, lambda){
#lambda = 1/mean(x)
#length(x) * log(lambda) - lambda * (sum(x))}
log.poissonlikelihood <- function(n, lambda){n*log(lambda) - lambda* sum(times)
}
plot(poissonlikelihood(times, lambda))
#likelihood <- function(x, lambda){
#lambda = 1/mean(x)
#length(x) * log(lambda) - lambda * (sum(x))}
log.poissonlikelihood <- function(n, lambda){n*log(lambda) - lambda* sum(times)
}
plot(log.poissonlikelihood(times, lambda))
#plot(likelihood(times, lambda), type="p", x = times)
#likelihood <- function(x, lambda){
#lambda = 1/mean(x)
#length(x) * log(lambda) - lambda * (sum(x))}
loglikelihood <- function (lambda, n) sum(dexp(n, lambda, log = TRUE))}
#likelihood <- function(x, lambda){
#lambda = 1/mean(x)
#length(x) * log(lambda) - lambda * (sum(x))}
loglikelihood <- function (lambda, n) sum(dexp(n, lambda, log = TRUE))
plot(loglikelihood(lambda, times))
#plot(likelihood(times, lambda), type="p", x = times)
#likelihood <- function(x, lambda){
#lambda = 1/mean(x)
#length(x) * log(lambda) - lambda * (sum(x))}
loglikelihood <- function (lambda, n) sum(dexp(n, lambda, log = TRUE))
plot(loglikelihood(times, lambda))
#plot(likelihood(times, lambda), type="p", x = times)
#likelihood <- function(x, lambda){
#lambda = 1/mean(x)
#length(x) * log(lambda) - lambda * (sum(x))}
loglikelihood <- function (lambda, n) sum(dexp(n, lambda, log = TRUE))
plot(loglikelihood(times, lambda), type="p", n = times)
#plot(likelihood(times, lambda), type="p", x = times)
#likelihood <- function(x, lambda){
#lambda = 1/mean(x)
#length(x) * log(lambda) - lambda * (sum(x))}
loglikelihood <- function (lambda, n) sum(dexp(n, lambda, log = TRUE))
plot(loglikelihood(times, lambda), type="p", x = times)
#likelihood <- function(x, lambda){
#lambda = 1/mean(x)
#length(x) * log(lambda) - lambda * (sum(x))}
loglikelihood <- function (lambda, n) (dexp(n, lambda, log = TRUE))
plot(loglikelihood(times, lambda), type="p", x = times)
#plot(likelihood(times, lambda), type="p", x = times)
#likelihood <- function(x, lambda){
#lambda = 1/mean(x)
#length(x) * log(lambda) - lambda * (sum(x))}
loglikelihood <- function (lambda, n) sum(dexp(n, lambda, log = TRUE))
plot(loglikelihood(times, lambda), type="p", x = times)
#likelihood <- function(x, lambda){
#lambda = 1/mean(x)
#length(x) * log(lambda) - lambda * (sum(x))}
loglikelihood <- function (lambda, n) (dexp(n, lambda, log = TRUE))
plot(loglikelihood(times, lambda), type="p", x = times)
#plot(likelihood(times, lambda), type="p", x = times)
#likelihood <- function(x, lambda){
#lambda = 1/mean(x)
#length(x) * log(lambda) - lambda * (sum(x))}
loglikelihood <- function (lambda, n) (dexp(n, lambda, log = TRUE))
plot(loglikelihood(lambda, times), type="p", x = times)
#plot(likelihood(times, lambda), type="p", x = times)
# To find the MLE, we minimize the negative of the log likelihood function
# (which finds the maximum). Using the Exponential Distribution function
# as part of R.
negloglikelihood <- function (lambda, n) -(dexp(n, lambda, log = TRUE))
optimize(negloglikelihood, lower = 0, upper = 10, n = times)
# To find the MLE, we minimize the negative of the log likelihood function
# (which finds the maximum). Using the Exponential Distribution function
# as part of R.
negloglikelihood <- function (lambda, n) -sum(dexp(n, lambda, log = TRUE))
optimize(negloglikelihood, lower = 0, upper = 10, n = times)
?dexp
#likelihood <- function(x, lambda){
#lambda = 1/mean(x)
#length(x) * log(lambda) - lambda * (sum(x))}
loglikelihood <- function (lambda, n) sum(dexp(n, lambda, log = TRUE))
plot(loglikelihood(lambda, times), type="p", x = times)
#likelihood <- function(x, lambda){
#lambda = 1/mean(x)
#length(x) * log(lambda) - lambda * (sum(x))}
#loglikelihood <- function (lambda, n) sum(dexp(n, lambda, log = TRUE))
#plot(loglikelihood(lambda, times), type="p", x = times)
#plot(likelihood(times, lambda), type="p", x = times)
dat <- data.frame(x=times, px=dexp(x, rate=(1/mean(times))))
#likelihood <- function(x, lambda){
#lambda = 1/mean(x)
#length(x) * log(lambda) - lambda * (sum(x))}
#loglikelihood <- function (lambda, n) sum(dexp(n, lambda, log = TRUE))
#plot(loglikelihood(lambda, times), type="p", x = times)
#plot(likelihood(times, lambda), type="p", x = times)
dat <- data.frame(x=times, px=dexp(times, rate=(1/mean(times))))
library(ggplot2)
ggplot(dat, aes(x=times, y=px)) + geom_line()
# To find the MLE, we minimize the negative of the log likelihood function
# (which finds the maximum). Using the Exponential Distribution function
# as part of R.
negloglikelihood <- function (lambda, n) sum(dexp(n, lambda, log = TRUE))
optimize(negloglikelihood, lower = 0, upper = 10, n = times, maximize=TRUE)
# To find the MLE, we minimize the negative of the log likelihood function
# (which finds the maximum). Using the Exponential Distribution function
# as part of R.
negloglikelihood <- function (lambda, n) sum(dexp(n, lambda, log = TRUE))
optimize(negloglikelihood, interval=c(0,100), n = times, maximize=TRUE)
# To find the MLE, we minimize the negative of the log likelihood function
# (which finds the maximum). Using the Exponential Distribution function
# as part of R.
negloglikelihood <- function (lambda, n) sum(dexp(n, lambda, log = TRUE))
optimize(negloglikelihood, lower = 0, upper = 10, n = times)
# To find the MLE, we minimize the negative of the log likelihood function
# (which finds the maximum). Using the Exponential Distribution function
# as part of R.
negloglikelihood <- function (lambda, n) -sum(dexp(n, lambda, log = TRUE))
optimize(negloglikelihood, lower = 0, upper = 10, n = times)
# The exponential distribution can be obtained with the dexp function,
# and can be plotted using the sample of x values (our vector "times")
# and processing them with that function. First, create a data frame with
# times and the likelihood function. The rate is given by lambda, which we
# found above as being equal to the reciprocal of the sample mean.
dat <- data.frame(x=times, px=dexp(times, rate=(1/mean(times)), log = TRUE))
library(ggplot2)
ggplot(dat, aes(x=times, y=px)) + geom_line()
# The exponential distribution can be obtained with the dexp function,
# and can be plotted using the sample of x values (our vector "times")
# and processing them with that function. First, create a data frame with
# times and the likelihood function. The rate is given by lambda, which we
# found above as being equal to the reciprocal of the sample mean.
dat <- data.frame(x=times, px=dexp(times, rate=(1/mean(times))))
library(ggplot2)
ggplot(dat, aes(x=times, y=px)) + geom_line()
# The exponential distribution can be obtained with the dexp function,
# and can be plotted using the sample of x values (our vector "times")
# and processing them with that function. First, create a data frame with
# times and the likelihood function. The rate is given by lambda, which we
# found above as being equal to the reciprocal of the sample mean.
dataframe <- data.frame(x=times, px=dexp(times, rate=(1/mean(times))))
library(ggplot2)
ggplot(dataframe, aes(x=times, y=px)) + geom_line()
knitr::opts_chunk$set(echo = TRUE)
?hist
knitr::opts_chunk$set(echo = TRUE)
pbinom(0,2,0.75)
pbinom(1,2,0.75)
pbinom(2,2,0.75)
# To simulate a draw for X and Y, create a data frame of the
# 100 samples from both X and Y
hundred <- data.frame(
x = runif(100, min= -1, max= 1),
y = runif(100, min= -1, max= 1)
)
# To calculate resulting values for D, make D an indicator value
# as part of the X and Y samples
hundred$d <- (hundred$x^2 + hundred$y^2 < 1)
# Plot the draws, assigning a different color to each point
# based on if it falls within the circle or not, passing d+1 into the color argument
plot(hundred$x, hundred$y, col= c("red", "black")[hundred$d+1], asp=1)
# Take the mean value of D and store it.
dbar <- mean(hundred$d)
dbar
# absolute value of the difference between expected value and sample mean
abs(dbar - pi/4)
# We take the previous simulation of n=100 and replicate it
# 10,000 times, calculating a sample mean each time.
# This will work best with a function.
replicator <- function(){
# take the same code as before, 1) make a data frame of the 100 samples from X and Y
hundred <- data.frame(
x = runif(100, min= -1, max= 1),
y = runif(100, min= -1, max= 1)
)
# 2) Calculate resulting values for D, make D an indicator value as part of the X and
# Y samples
hundred$d <- (hundred$x^2 + hundred$y^2 < 1)
# 3) use code from part f to find sample mean
dbar = mean(hundred$d)
return (dbar)
}
# We have the replicator function, now time to actually replicate it
sample_mean_study <- replicate(10000, replicator())
# And finally, make a histogram of the 10,000 sample means
hist(sample_mean_study, xlab = sample mean, main= "Histogram of DBar sample means")
# We take the previous simulation of n=100 and replicate it
# 10,000 times, calculating a sample mean each time.
# This will work best with a function.
replicator <- function(){
# take the same code as before, 1) make a data frame of the 100 samples from X and Y
hundred <- data.frame(
x = runif(100, min= -1, max= 1),
y = runif(100, min= -1, max= 1)
)
# 2) Calculate resulting values for D, make D an indicator value as part of the X and
# Y samples
hundred$d <- (hundred$x^2 + hundred$y^2 < 1)
# 3) use code from part f to find sample mean
dbar = mean(hundred$d)
return (dbar)
}
# We have the replicator function, now time to actually replicate it
sample_mean_study <- replicate(10000, replicator())
# And finally, make a histogram of the 10,000 sample means
hist(sample_mean_study, xlab = "sample mean", main= "Histogram of DBar sample means")
# Find the standard deviation of the sample mean study
sample_mean_study_sd <- sd(sample_mean_study)
sample_mean_study_sd
# absolute value of difference between expected sd and sample mean study sd
abs(sample_mean_study_sd - 0.4105)
?ggplot
# The exponential distribution can be obtained with the dexp function,
# and can be plotted using the sample of x values (our vector "times")
# and processing them with that function. First, create a data frame with
# "times" and the likelihood function. The rate is given by lambda, which we
# found above as being equal to the reciprocal of the sample mean.
dataframe <- data.frame(x=times, px=dexp(times, rate=(1/mean(times))))
library(ggplot2)
ggplot(dataframe, aes(x=times, y=P(x))) + geom_line()
# The exponential distribution can be obtained with the dexp function,
# and can be plotted using the sample of x values (our vector "times")
# and processing them with that function. First, create a data frame with
# "times" and the likelihood function. The rate is given by lambda, which we
# found above as being equal to the reciprocal of the sample mean.
dataframe <- data.frame(x=times, px=dexp(times, rate=(1/mean(times))))
library(ggplot2)
ggplot(dataframe, aes(x=times)) + geom_line()
# The exponential distribution can be obtained with the dexp function,
# and can be plotted using the sample of x values (our vector "times")
# and processing them with that function. First, create a data frame with
# "times" and the likelihood function. The rate is given by lambda, which we
# found above as being equal to the reciprocal of the sample mean.
dataframe <- data.frame(x=times, px=dexp(times, rate=(1/mean(times))))
library(ggplot2)
ggplot(dataframe, aes(x=times, y)) + geom_line()
# The exponential distribution can be obtained with the dexp function,
# and can be plotted using the sample of x values (our vector "times")
# and processing them with that function. First, create a data frame with
# "times" and the likelihood function. The rate is given by lambda, which we
# found above as being equal to the reciprocal of the sample mean.
dataframe <- data.frame(x=times, px=dexp(times, rate=(1/mean(times))))
library(ggplot2)
ggplot(dataframe, aes(x=times, y=px)) + geom_line()
?ggplot
knitr::opts_chunk$set(echo = TRUE)
gpa = read.csv('gpa1.rdata')
gpa = read.csv("gpa1.rdata")
gpa <- read.csv("gpa1.rdata", header = TRUE, sep = ",", encoding="latin1")
gpa <- read.csv("gpa1.rdata", header = TRUE, sep = ",", encoding="latin1")
gpa <- read.csv("gpa1.rdata", header = TRUE, sep = ",")
summary (gpa)
cols(gpa)
ls(gpa)
gpa <- read.csv("gpa1.rdata")
summary (gpa)
ls(gpa)
summary(gpa$RDX2)
gpa <- read.csv("gpa1.rdata")
summary (gpa)
ls(gpa)
summary(gpa$RDX2)
colnames(gpa)
summary(gpa$RDX2)
load("/Users/alexwest/Desktop/stats_class/submissions/unit_8/gpa1.rdata")
knitr::opts_chunk$set(echo = TRUE)
gpa <- read.csv("gpa1.rdata")
summary (gpa)
ls(gpa)
colnames(gpa)
summary(gpa$RDX2)
head(gpa)
(gpa)
gpa <- read.csv("gpa1.rdata")
summary (gpa)
ls(gpa)
colnames(gpa)
summary(gpa$RDX2)
(gpa)
gpa <- read.csv("gpa1.rdata")
summary (gpa)
ls(gpa)
colnames(gpa)
summary(gpa$RDX2)
(gpa)
gpa$skipped
gpa <- load("gpa1.rdata")
gpa <- load("gpa1.rdata")
summary (gpa)
ls(gpa)
summary (gpa)
colnames(gpa)
summary(gpa$RDX2)
head(gpa)
load.Rdata( filename="gpa1.rdata", "gpa" )
setwd(/Desktop/stats_class/submissions/unit_8)
setwd(Desktop/stats_class/submissions/unit_8)
setwd(/users/alexwest/Desktop/stats_class/submissions/unit_8)
setwd(users/alexwest/Desktop/stats_class/submissions/unit_8)
setwd(./Desktop/stats_class/submissions/unit_8)
setwd(Users/alexwest/Desktop/stats_class/submissions/unit_8)
getwd()
setwd("Users/alexwest/Desktop/stats_class/submissions/unit_8")
setwd("/Desktop/stats_class/submissions/unit_8")
knitr::opts_chunk$set(echo = TRUE)
load("gpa1.rdata")
nrow(data)
summary(data$skipped)
sd(data$skipped)
ls(data)
hist(data$skipped, breaks = 0:6 - 0.5, main= "Histogram of 'Skipped' Variable")
t.test(data$skipped, mu = 1)
# Find the sample mean for the 'skipped' variable
mean_skipped <- mean(data$skipped)
# Define mu, or the mean we are testing against for
# the null hypothesis
mu <- 1
# Define n
n <- length(data$skipped)
# Define the t value given the data
t <- (mean_skipped - mu)/(sd(data$skipped)/sqrt(n))
# Print the t value
print(paste("t value is:", t))
# Define the degrees of freedom (n-1)
degfrdm <- n - 1
# Find p value using the pt function and inserting
# the values for t and degrees of freedom we just calculated
p <- 2 * pt(t, degfrdm, lower = FALSE)
# Print the p value
print(paste("P value is:", p))
head(mtcars)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
# We use the stargazer package to display nice regression tables.
library(stargazer)
install.packages("stargazer")
# We use the stargazer package to display nice regression tables.
library(stargazer)
load("GPA1.rdata")
head(data)
# create the scatterplot
plot(jitter(data$ACT), jitter(data$colGPA), xlab = "ACT score", ylab = "College GPA", main = "College GPA versus ACT score")
# fit the linear model
(model1 = lm(colGPA ~ ACT, data = data))
# Add regression line to scatterplot
abline(model1)
plot(model1, which = 5)
# create new variable
ACT_with_error = data$ACT
# change one data point
ACT_with_error[5] = 80
# fit a new linear model
model1_with_error = lm(data$colGPA ~ ACT_with_error)
# visualize the data with the error and the new ols line
plot(jitter(ACT_with_error), jitter(data$colGPA), xlab = "ACT score", ylab = "College GPA", main = "College GPA versus ACT score including Error")
# Add regression line to scatterplot
abline(model1_with_error)
plot(model1_with_error, which=5, main = "GPA Data with Error Introduced")
summary(data$hsGPA)
hist(data$hsGPA, breaks = 20, main = "High School GPA", xlab = NULL)
library(car)
scatterplotMatrix(data[,c("colGPA", "ACT", "hsGPA")], diagonal = "histogram")
summary(data$hsGPA)
hist(data$hsGPA, breaks = 20, main = "High School GPA", xlab = NULL)
library(carData)
scatterplotMatrix(data[,c("colGPA", "ACT", "hsGPA")], diagonal = "histogram")
# left side of tilde is college GPA, plus does not
# equal addition, it's the multivariate signal to R
# data frame is named data
(model2 = lm(colGPA ~ ACT + hsGPA, data = data))
model2$coefficients
summary(model1)$r.square
summary(model2)$r.square
AIC(model1)
AIC(model2)
library(stargazer)
stargazer(model1, model2, type = "latex",
report = "vc", # Don't report errors, since we haven't covered them
title = "Linear Models Predicting College GPA",
keep.stat = c("rsq", "n"), # want to see the rsquared and n statistics
omit.table.layout = "n") # Omit more output related to errors
stargazer(model1, model2, type = "text",
report = "vc", # Don't report errors, since we haven't covered them
title = "Linear Models Predicting College GPA",
keep.stat = c("rsq", "n"), # want to see the rsquared and n statistics
omit.table.layout = "n") # Omit more output related to errors
stargazer(model1, model2, type = "latex",
report = "vc", # Don't report errors, since we haven't covered them
title = "Linear Models Predicting College GPA",
keep.stat = c("rsq", "n"), # want to see the rsquared and n statistics
omit.table.layout = "n") # Omit more output related to errors
stargazer(model1, model2, type = "text",
report = "vc", # Don't report errors, since we haven't covered them
title = "Linear Models Predicting College GPA",
keep.stat = c("rsq", "n"), # want to see the rsquared and n statistics
omit.table.layout = "n") # Omit more output related to errors
setwd("~/Desktop/stats_class/submissions/lab_3")
knitr::opts_chunk$set(echo = TRUE)
data <- read.csv("crime_v2.csv")
load("crime_v2.csv")
pwd
setwd("~/Desktop/stats_class/submissions/lab_3/w203_proj3/data")
data <- read.csv("crime_v2.csv")
data <- read.csv("..data/crime_v2.csv")
setwd("~/Desktop/stats_class/submissions/lab_3/w203_proj3/analysis")
knitr::opts_chunk$set(echo = TRUE)
data <- read.csv("..data/crime_v2.csv")
setwd("~/Desktop/stats_class/submissions/lab_3/w203_proj3/data")
data <- read.csv("crime_v2.csv")
gwd()
getwd()
getwd()
setwd("/Users/alexwest/Desktop/stats_class/submissions/lab_3/w203_proj3/data"")
#data <- read.csv("crime_v2.csv")
getwd()
setwd("/Users/alexwest/Desktop/stats_class/submissions/lab_3/w203_proj3/data")
getwd()
setwd("/Users/alexwest/Desktop/stats_class/submissions/lab_3/w203_proj3/data")
data <- read.csv("crime_v2.csv")
summary(data)
ls(data)
desc
desc(data)
desc
ls(data)
head(data)
